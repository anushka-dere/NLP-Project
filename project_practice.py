# -*- coding: utf-8 -*-
"""project practice.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D1uKvrjXmhJzGtfKnGlWN1G4gvA7sPov
"""

import pandas as pd
import numpy as np

import warnings
warnings.filterwarnings('ignore')
pd.set_option('display.max_rows', None)

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
#from sklearn.naive_bayes import MultinominalNB
from sklearn.metrics import (accuracy_score, classification_report, plot_confusion_matrix)

df=pd.read_csv("Product_details.csv")
df.head()



"""# `Natural Language Proccessing` :-
 In order to perform an analysis of text data, data preprocessing is first done to transform text into a data format that can be used in machine learning.

### 1.) Basic Cleaning and Tokenization
"""

text = df.Product_Description[1].lower()
text

"""#### 1.1) Removing Special Character such as Punctuation"""

import re
text = re.sub(r'([^A-Za-z0-9|\s|[:punct:]]*)', '', text)
text

# Repacing all character that are not letters (a-z or A-Z) or# with space character.
text = text.replace('[^a-zA-Z#]',' ').replace('quot','')
text

# By seeing above and thinking logically we can say that words havinng character less than 3, it should be replaced.

text = ' '.join([i  for i in text.split() if len(i)>3])
text

"""#### 1.2) Tokenization by spliting the string - transforming into a list of words."""

text = text.split()
text

"""### 2.) Lemitization and Stopwords:-

#### 2.1) Stopwords Removal:-
"""

# Removing the stopword to reduce the dimensionality of the data down to only the words that conntain important information.
import nltk
nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words("english")

#Remove words related to the conference that appear accross all sentiments
# and terms specific to review plateform

sxsw = ['sxsw', 'sxswi', 'link', 'quot', 'rt', 'amp', 'mention', 'apple', 'google', 'iphone', 'ipad', 
        'ipad2', 'austin', 'today', 'quotroutearoundquot', 'rtmention', 'store', 'doesnt', 'theyll']

stopwords.extend(sxsw)

text = [word for word in text if word not in stopwords]

text

"""#### 2.2) Lemitization:-
To reduce each word to  its most basic form, such as systems to system.
"""

from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
nltk.download('omw-1.4')
lemmatizer = nltk.stem.WordNetLemmatizer()

text = [lemmatizer.lemmatize(word) for word in text]
text

# Put all the words  back together
text = ' '.join(text)
text

"""### Applying all the above steps to whole Product_Description Column."""

# Create a function that compiles all the steps taken above
def preprocess(text):
    """
    This function performs basic data cleaning, tokenization, lemmatization, and stopword removal on the input text.
    
    Args:
        text (str): The input text to be preprocessed.
        
    Returns:
        str: The preprocessed text.
    """
    
    # Convert text to lowercase
    text = text.apply(lambda x: x.lower())
    
    # Remove special characters and digits
    text = text.apply(lambda x: re.sub(r'([^A-Za-z|\s|[:punct:]]*)', '', x))
    
    # Replace certain characters and words with spaces
    text = text.apply(lambda x: x.replace('[^a-zA-Z#]', ' ').replace('quot', '').replace(':', '').replace('sxsw', ''))
    
    # Remove words that are shorter than 2 characters
    text = text.apply(lambda x: ' '.join([i for i in x.split() if len(i) > 1]))
    
    # Tokenize the text
    text = text.apply(lambda x: x.split())
    
    # Lemmatize the tokens
    text = text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])
    
    # Remove stopwords
    text = text.apply(lambda x: [word for word in x if word not in stopwords])
    
    # Join the preprocessed tokens back into a single string
    text = text.apply(lambda x: ' '.join(x))
    
    return text

df["Product_Description"] = preprocess(df["Product_Description"])

df

"""#### 2.3 Vectorization
Once the text data is cleaned, the last step is to convert it to vectors. A basic way to vectorize text data is using the `CountVectorizer` which counts the number of times each word appears.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

X = tfidf.fit_transform(df["Product_Description"]).toarray()

X = pd.DataFrame(X,columns = tfidf.get_feature_names())

print(X.shape)

X_feature = pd.DataFrame(X,df.Product_Type) # Feature 
Y = df.Sentiment

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier



# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_feature, Y, test_size=0.2, random_state=42)

# Train a classifier on the data
gb = XGBClassifier()
gb.fit(X_train, y_train)

# # Test the classifier on the original test set
y_pred = gb.predict(X_test)

from sklearn.metrics import classification_report

# ... # your performance metric
print(classification_report(y_test,y_pred))

import pickle

filename = "xgb.sav"
pickle.dump(gb, open(filename,'wb'))

loaded_model= pickle.load(open('xgb.sav','rb'))

y_pred

